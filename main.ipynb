{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dgoZ8PUD-UZ"
      },
      "source": [
        "# Part I - Prepare Base Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZbLsPs3EA4R"
      },
      "source": [
        "## 1. Set up Environment and Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP6DQpz0D5m0"
      },
      "outputs": [],
      "source": [
        "! pip install  sentencepiece accelerate safetensors datasets torchvision scikit-optimize openai ratelimiter scikits.bootstrap arch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KKQmXHmxEDdh"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import json\n",
        "import openai\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import pipeline\n",
        "from google.colab import drive\n",
        "from urllib.parse import urlparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxH9xT_0GBSv",
        "outputId": "67550be5-3105-4dc9-f818-3a176249bf99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "with open('file_path/file.pkl', 'rb') as file:\n",
        "    full_data = pickle.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDTYtdiEJvRK"
      },
      "source": [
        "## 2. Extract Domains and Label Domain Credibility"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AinTW4JDYEBm"
      },
      "source": [
        "### 2.1 Extract Domains from URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IM6A6OWZIxFz"
      },
      "outputs": [],
      "source": [
        "def extract_domain(df):\n",
        "    df['urls'] = df['urls'].fillna('')\n",
        "    df['urls'] = df['urls'].astype(str)\n",
        "    df = df.assign(urls=df['urls'].str.split(';')).explode('urls')\n",
        "    df['domain'] = df['urls'].apply(lambda x: urlparse(x).netloc if isinstance(x, str) else '')\n",
        "    df['domain'] = df['domain'].str.replace('www.', '')\n",
        "\n",
        "    return df\n",
        "\n",
        "for df_name, dataframe in full_data.items():\n",
        "    full_data[df_name] = extract_domain(dataframe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hccLEXFX-WlA"
      },
      "source": [
        "### 2.2 Add Credibility Scores to Original Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJjyHTV2adb0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "credibility_ratings = pd.read_csv('file_path/file.csv')\n",
        "credibility_ratings = credibility_ratings[['url', 'rating',]]\n",
        "credibility_ratings = credibility_ratings.reset_index(drop=True)\n",
        "\n",
        "def merge_credibility_rating(df, credibility_ratings):\n",
        "    merged_data = pd.merge(df, credibility_ratings[['url', 'rating']], left_on='domain', right_on='url', how='left')\n",
        "    merged_data = merged_data.drop(columns='url')\n",
        "    return merged_data\n",
        "\n",
        "full_data['covid'] = merge_credibility_rating(full_data['covid'], credibility_ratings)\n",
        "full_data['climate'] = merge_credibility_rating(full_data['climate'], credibility_ratings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DV0VNq7Dav6z"
      },
      "source": [
        "### 2.3 Split Datasets in Low-Credibility and High-Credibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-Psx6Vl54DT"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "def separate_by_trust_ratings(data, name):\n",
        "    original_data_length = len(data)\n",
        "    low_cred, high_cred = data[data['rating'] <= 0.4], data[data['rating'] >= 0.6]\n",
        "\n",
        "    print(f\"Number of rows in {name} low credibility: {len(low_cred)}\")\n",
        "    print(f\"Number of rows in {name} high credibility: {len(high_cred)}\")\n",
        "\n",
        "    low_cred_percentage = (len(low_cred) / original_data_length) * 100\n",
        "    print(f\"Percentage of {name} data that is low credibility: {low_cred_percentage:.2f}%\")\n",
        "\n",
        "    return low_cred, high_cred\n",
        "\n",
        "covid_low_cred, covid_high_cred = separate_by_trust_ratings(full_data['covid'], 'COVID-19')\n",
        "climate_low_cred, climate_high_cred = separate_by_trust_ratings(full_data['climate'], 'Climate Change')\n",
        "\n",
        "data_dict = {\n",
        "    'covid_low_cred': covid_low_cred,\n",
        "    'covid_high_cred': covid_high_cred,\n",
        "    'climate_low_cred': climate_low_cred,\n",
        "    'climate_high_cred': climate_high_cred\n",
        "}\n",
        "\n",
        "with open('file_path/file.pkl', 'wb') as f:\n",
        "    pickle.dump(data_dict, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dla82RAofhuI"
      },
      "source": [
        "# Part II  - Stratification Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVagOxnfUJQ1",
        "outputId": "17bad87e-e068-46c3-fa27-3475dbb2c9a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "with open('file_path/file.pkl', 'rb') as file:\n",
        "    annotated_data = pickle.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDw5XslsgKBh"
      },
      "source": [
        "## 3.1 Engagement Levels and Followers Clustering with Quantile Based Discretization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBjYz4Xsxq-n"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import KBinsDiscretizer\n",
        "\n",
        "def qb_discretization(annotated_data, process_col, n_bins, strategy='quantile'):\n",
        "    def relabel_bins(df, process_col, n_bins, strategy, label_name):\n",
        "        kbins = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy=strategy)\n",
        "        bin_labels = kbins.fit_transform(df[[process_col]])\n",
        "        return bin_labels.ravel().astype(int)\n",
        "\n",
        "\n",
        "    label_name = process_col.split('_')[0] + '_cluster'\n",
        "\n",
        "    data_groups = {'climate': pd.concat([df.assign(source=key) for key, df in annotated_data.items() if 'climate' in key]),\n",
        "                   'covid': pd.concat([df.assign(source=key) for key, df in annotated_data.items() if 'covid' in key])}\n",
        "\n",
        "    result_dict = {}\n",
        "    for group_name, group_df in data_groups.items():\n",
        "        group_df[process_col] = group_df[process_col].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
        "        group_df[process_col] = np.log1p(group_df[process_col])\n",
        "        group_df[label_name] = relabel_bins(group_df, process_col, n_bins, strategy, label_name)\n",
        "\n",
        "        for key in [k for k in annotated_data.keys() if group_name in k]:\n",
        "            key_df = group_df[group_df['source'] == key]\n",
        "            result_dict[key] = key_df[['id', label_name, process_col]].to_dict(orient='records')\n",
        "            cluster_counts = key_df[label_name].value_counts()\n",
        "            cluster_means = key_df.groupby(label_name)[process_col].mean()\n",
        "            print(f\"Mean value of '{process_col}' in each cluster for '{key}':\\n{cluster_means}\\n\")\n",
        "\n",
        "    return result_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opE8XT5V6Uub"
      },
      "outputs": [],
      "source": [
        "## Apply to Engagement Data\n",
        "for key in annotated_data:\n",
        "    annotated_data[key]['engagement_merged'] = annotated_data[key][['retweet_count', 'reply_count', 'like_count', 'quote_count']].apply(pd.to_numeric, errors='coerce').fillna(0).sum(axis=1)\n",
        "\n",
        "results_engagement = qb_discretization(annotated_data, process_col='engagement_merged', n_bins=9)\n",
        "with open('file_path/file.json', 'w') as f:\n",
        "    json.dump(results_engagement, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G47c8lWp6Y34"
      },
      "outputs": [],
      "source": [
        "## Apply to Followers Data\n",
        "results_followers = qb_discretization(annotated_data, process_col='followers_count',n_bins=4)\n",
        "\n",
        "with open('file_path/file.json', 'w') as f:\n",
        "    json.dump(results_followers, f, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePR-vlOUTiE5"
      },
      "source": [
        "## 3.2 Toxicity Scoring With Perspective API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANEKR9-saAae"
      },
      "outputs": [],
      "source": [
        "import aiohttp\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "\n",
        "def set_perspective_key():\n",
        "    with open(\"path_to_key/key.json\") as f:\n",
        "        return json.load(f).get(\"PERSPECTIVE_API_KEY\")\n",
        "\n",
        "async def classify_text(session, api_key, text, bucket, max_retries=5):\n",
        "    initial_wait_time, retry_count = 1, 0\n",
        "    payload = {\n",
        "        \"comment\": {\"text\": text},\n",
        "        \"languages\": [\"en\"],\n",
        "        \"requestedAttributes\": {\"TOXICITY\": {}}\n",
        "    }\n",
        "    while bucket[0] <= 0 or retry_count < max_retries:\n",
        "        if bucket[0] > 0:\n",
        "            bucket[0] -= 1\n",
        "            try:\n",
        "                async with session.post(\"https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze\", params={\"key\": api_key}, json=payload, timeout=None) as response:\n",
        "                    return await response.json()\n",
        "            except Exception as e:\n",
        "                retry_count += 1\n",
        "                print(f\"Request failed, waiting for {initial_wait_time * (2 ** retry_count)} seconds before retrying. Exception: {e}\")\n",
        "                await asyncio.sleep(initial_wait_time * (2 ** retry_count))\n",
        "        else:\n",
        "            await asyncio.sleep(1)\n",
        "    print(f\"Failed to get response after {max_retries} retries.\")\n",
        "\n",
        "async def refill_bucket(bucket, fill_rate=700):\n",
        "    while True:\n",
        "        bucket[0] = fill_rate\n",
        "        await asyncio.sleep(60)\n",
        "\n",
        "async def main(df_name, dataframe, json_data):\n",
        "    print(f\"Processing DataFrame: {df_name}\")\n",
        "    api_key, token_bucket = set_perspective_key(), [700]\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = [classify_text(session, api_key, text, token_bucket) for text in dataframe['text'].to_list()]\n",
        "        refill_task = asyncio.create_task(refill_bucket(token_bucket))\n",
        "        dataframe_results = [{\"id\": tweet_id, \"toxicity\": None} for tweet_id in dataframe['id']]\n",
        "        for index, future in enumerate(asyncio.as_completed(tasks)):\n",
        "            result = await future\n",
        "            if result and 'attributeScores' in result and 'TOXICITY' in result['attributeScores'] and 'summaryScore' in result['attributeScores']['TOXICITY'] and 'value' in result['attributeScores']['TOXICITY']['summaryScore']:\n",
        "                toxicity_score = result['attributeScores']['TOXICITY']['summaryScore']['value']\n",
        "                dataframe_results[index][\"toxicity\"] = toxicity_score\n",
        "            else:\n",
        "                print(f\"No 'attributeScores' in result for tweet_id {dataframe_results[index]['id']}. Full result: {result}\")\n",
        "        json_data[df_name] = dataframe_results\n",
        "        refill_task.cancel()\n",
        "\n",
        "async def process_dataframes(dataframes):\n",
        "    json_data = {}\n",
        "    for df_name, dataframe in dataframes.items():\n",
        "        await main(df_name, dataframe, json_data)\n",
        "    with open('file_path/file.json', 'w') as f:\n",
        "        json.dump(json_data, f, indent=4)\n",
        "\n",
        "loop = asyncio.get_event_loop()\n",
        "loop.run_until_complete(process_dataframes(annotated_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sApGya9d6tg",
        "outputId": "e6eead84-c294-485b-edc1-321eca68a5da"
      },
      "outputs": [],
      "source": [
        "# Use kmeans clustering to divide toxicity in clusters\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "with open('file_path/file.json', 'r') as f:\n",
        "    data_json = json.load(f)\n",
        "\n",
        "datasets = {'climate': pd.DataFrame(), 'covid': pd.DataFrame()}\n",
        "for key, df in ((k, pd.DataFrame(v)) for k, v in data_json.items()):\n",
        "    datasets['climate' if 'climate' in key else 'covid'] = pd.concat([datasets['climate' if 'climate' in key else 'covid'], df], ignore_index=True)\n",
        "\n",
        "kmeans = KMeans(n_clusters=3, random_state=18)\n",
        "for name, df in datasets.items():\n",
        "    df['toxicity_cluster'] = kmeans.fit_predict(df[['toxicity']])\n",
        "    cluster_map = {old: new for new, old in enumerate(df.groupby('toxicity_cluster')['toxicity'].mean().sort_values().index)}\n",
        "    df['toxicity_cluster'] = df['toxicity_cluster'].map(cluster_map)\n",
        "\n",
        "for key in data_json:\n",
        "    dataframe = datasets['climate'] if 'climate' in key else datasets['covid']\n",
        "    for item in data_json[key]:\n",
        "        item['toxicity_cluster'] = int(dataframe[dataframe['toxicity'] == item['toxicity']]['toxicity_cluster'].values[0])\n",
        "\n",
        "with open('file_path/file.json', 'w') as f:\n",
        "    json.dump(data_json, f, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srxJj9QRIjk7"
      },
      "source": [
        "## 3.3 Political Bias with GPT-4 API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ABtWZSYDIusW"
      },
      "outputs": [],
      "source": [
        "import ast\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from ratelimiter import RateLimiter\n",
        "\n",
        "def delayed_chat_completion(delay_in_seconds: float = 1, **kwargs):\n",
        "    time.sleep(delay_in_seconds)\n",
        "    return openai.ChatCompletion.create(**kwargs)\n",
        "\n",
        "def set_openai_api_key():\n",
        "    with open(\"path_to_key/key.json\") as f:\n",
        "        openai_keys_obj = json.load(f)\n",
        "    openai.api_key = openai_keys_obj.get(\"OPENAI_API_KEY\")\n",
        "    openai.organization = openai_keys_obj.get(\"ORGANIZATION\")\n",
        "\n",
        "def get_label_constructor(system_message, user_message, default_response_dict):\n",
        "    def get_label(domain, example):\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_message},\n",
        "            {\"role\": \"user\", \"content\": user_message.format(domain=domain, example=example)}\n",
        "        ]\n",
        "        rate_limit_per_minute = 60 ### Change this according to your rate limit\n",
        "        delay = 60.0 / rate_limit_per_minute\n",
        "\n",
        "        response = delayed_chat_completion(\n",
        "            delay_in_seconds=delay,\n",
        "            model=\"gpt-4\",\n",
        "            messages=messages,\n",
        "            max_tokens=120,\n",
        "            temperature=0\n",
        "        )\n",
        "        reply = response['choices'][0]['message']['content'].replace(\"Assistant:\", \"\").strip()\n",
        "\n",
        "        try:\n",
        "            reply_dict = ast.literal_eval(reply)\n",
        "        except SyntaxError:\n",
        "            reply_dict = default_response_dict(domain, reply)\n",
        "        return domain, reply_dict\n",
        "    return get_label\n",
        "\n",
        "def classify_domain(df1, df2, system_message, user_message, example, default_response_dict, output_file):\n",
        "    set_openai_api_key()\n",
        "\n",
        "    merged_df = pd.concat([df1, df2])\n",
        "    merged_df = merged_df[pd.notnull(merged_df['domain']) & (merged_df['domain'] != '')]\n",
        "    unique_domains = merged_df['domain'].unique()\n",
        "\n",
        "    label_dict = {}\n",
        "    rate_limiter = RateLimiter(max_calls=55, period=60)\n",
        "\n",
        "    get_label = get_label_constructor(system_message, user_message, default_response_dict)\n",
        "\n",
        "    try:\n",
        "        with ThreadPoolExecutor(max_workers=55) as executor:\n",
        "            futures = []\n",
        "\n",
        "            def submit_task(domain):\n",
        "                with rate_limiter:\n",
        "                    return executor.submit(get_label, domain, example)\n",
        "\n",
        "            for domain in unique_domains:\n",
        "                futures.append(submit_task(domain))\n",
        "\n",
        "            for future in as_completed(futures):\n",
        "                domain, label = future.result()\n",
        "                label_dict[domain] = label\n",
        "    except Exception as e:\n",
        "        print(f\"Exception occurred: {str(e)}\")\n",
        "\n",
        "    with open(output_file, 'w') as json_file:\n",
        "        json.dump(label_dict, json_file, indent=4)\n",
        "\n",
        "    return label_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ah-Vix3eIvS2"
      },
      "outputs": [],
      "source": [
        "def default_response_dict_bias(domain, reply):\n",
        "    return {'url': domain, 'political_bias': -1, 'reason': reply}\n",
        "\n",
        "system_message_bias = \"You are an assistant to help the user determine the political bias of websites.\"\n",
        "user_message_bias = \"Rate the political bias of the website '{domain}' using the labels 'far-right', 'right', 'no bias', 'left', 'far-left'. If the assistant has no knowledge of the website, it will return a rating of -1. Assistant should only provide the result in JSON format, do not provide anything else. Here is an example: {example}\"\n",
        "example_bias = {\n",
        "    'url': 'example.com',\n",
        "    'political_bias': 'no_bias',\n",
        "    'reason': 'The example website is known to have no political bias.'\n",
        "}\n",
        "output_file_bias = 'file_path/file.json'\n",
        "\n",
        "bias_ratings = classify_domain(full_data['covid'], full_data['climate'], system_message_bias, user_message_bias, example_bias, default_response_dict_bias, output_file_bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87yDt4HS1HNI"
      },
      "source": [
        "# Part III - Amplification Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiMKluWcDHoS"
      },
      "source": [
        "## 4. Amplification Analysis with Bootstrapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ool6OsqyDPv7"
      },
      "source": [
        "### 4.1 Checkpoint - Reload Annotated Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHFYfjTzC-WU",
        "outputId": "48895953-d86f-4efc-8a2a-59cfc98b2511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "with open('file_path/file.pkl', 'rb') as file:\n",
        "    annotated_data = pickle.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epkuqWSbAhAr"
      },
      "source": [
        "### 4.2 Hydrate Datasets with Stratification Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DW1mQ_Cx9CKs"
      },
      "outputs": [],
      "source": [
        "def hydrate_data(full_data, json_file_path, target_col, match_col='id', match_func=None):\n",
        "    with open(json_file_path) as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    if match_func is not None:\n",
        "        for df in full_data.values():\n",
        "            df[target_col] = df[match_col].map(match_func).astype(str)\n",
        "    else:\n",
        "        grouping_var = {key: pd.DataFrame(value) for key, value in data.items()}\n",
        "        for key, df in full_data.items():\n",
        "            other_df = grouping_var.get(key)\n",
        "            if other_df is not None and {match_col, target_col}.issubset(other_df.columns):\n",
        "                mapping = other_df.set_index(match_col)[target_col].to_dict()\n",
        "                df[target_col] = df[match_col].map(mapping)\n",
        "\n",
        "    if target_col == 'political_bias':\n",
        "        for df in full_data.values():\n",
        "            df[target_col] = df[target_col].replace({'None': 'no_bias', '-1': 'unknown'})\n",
        "\n",
        "    return full_data\n",
        "\n",
        "# Load the political bias data\n",
        "with open(\"file_path/file.json\") as f:\n",
        "    political_bias_data = json.load(f)\n",
        "\n",
        "# Hydrate the data with different stratification variables\n",
        "hydrations = [\n",
        "    ('file_path/file.json', 'toxicity_cluster'),\n",
        "    ('file_path/file.json', 'engagement_cluster'),\n",
        "    ('file_path/file.json', 'followers_cluster'),\n",
        "    ('file_path/file.json', 'political_bias', 'domain', lambda url: political_bias_data.get(url, {}).get('rating'))\n",
        "]\n",
        "\n",
        "for hydration in hydrations:\n",
        "    annotated_data = hydrate_data(annotated_data, *hydration)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_g-fKFZIRXl"
      },
      "source": [
        "### 4.3 Run Baseline Comparisons With Bootstrapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBz0U7m_E-H-"
      },
      "source": [
        "#### 4.3.1  Define Baseline Stratification with Bootstrapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LG1YTEe5jat5"
      },
      "outputs": [],
      "source": [
        "from arch.bootstrap import IIDBootstrap\n",
        "from sklearn.utils import resample\n",
        "from scipy.stats import iqr\n",
        "\n",
        "def create_slices(data, additional_var=None):\n",
        "    if additional_var is None:\n",
        "        return {(engagement, followers): data[(data[\"engagement_cluster\"] == engagement) & (data[\"followers_cluster\"] == followers)][\"impressions_count\"]\n",
        "                for engagement in data[\"engagement_cluster\"].unique()\n",
        "                for followers in data[\"followers_cluster\"].unique()}\n",
        "    else:\n",
        "        return {(engagement, followers, new_var): data[(data[\"engagement_cluster\"] == engagement) & (data[\"followers_cluster\"] == followers) & (data[additional_var] == new_var)][\"impressions_count\"]\n",
        "                for engagement in data[\"engagement_cluster\"].unique()\n",
        "                for followers in data[\"followers_cluster\"].unique()\n",
        "                for new_var in data[additional_var].unique()}\n",
        "\n",
        "def calculate_difference(sample_1, sample_2, percentage=False):\n",
        "    mean_1, mean_2 = np.mean(sample_1), np.mean(sample_2)\n",
        "    if percentage and not np.isclose(mean_2, 0):\n",
        "        return (mean_1 - mean_2) / mean_2 * 100\n",
        "    else:\n",
        "        return mean_1 - mean_2\n",
        "\n",
        "def bootstrap_iteration(data_slices, iteration, keys, stratum_boot_differences, percentage=False):\n",
        "    boot_cred = {cred_key: [] for cred_key in keys}\n",
        "\n",
        "    for key in data_slices[keys[0]]:\n",
        "        sample_size = min(len(data_slices[keys[0]][key]), len(data_slices[keys[1]][key]))\n",
        "        samples = {cred_key: resample(data_slices[cred_key][key], n_samples=sample_size) for cred_key in keys}\n",
        "        for cred_key in keys:\n",
        "            boot_cred[cred_key].extend(samples[cred_key])\n",
        "\n",
        "        stratum_difference = calculate_difference(samples[keys[0]], samples[keys[1]], percentage)\n",
        "        stratum_boot_differences[key].append(stratum_difference)\n",
        "\n",
        "    overall_difference = calculate_difference(boot_cred[keys[0]], boot_cred[keys[1]], percentage)\n",
        "    return overall_difference\n",
        "\n",
        "def stratified_bootstrapping(annotated_data, high_cred_key, low_cred_key, n_iterations, main_var_name, seed, ci=0.95, percentage=False, additional_var=None):\n",
        "    np.random.seed(seed)\n",
        "    data_slices = {cred_key: create_slices(annotated_data[cred_key], additional_var) for cred_key in [high_cred_key, low_cred_key]}\n",
        "    stratum_boot_differences = {key: [] for key in data_slices[high_cred_key].keys()}\n",
        "\n",
        "    boot_differences = []\n",
        "    for iteration in range(n_iterations):\n",
        "        overall_difference = bootstrap_iteration(data_slices, iteration, [low_cred_key, high_cred_key], stratum_boot_differences, percentage)\n",
        "        boot_differences.append(overall_difference)\n",
        "\n",
        "    results_df = pd.DataFrame({main_var_name: boot_differences})\n",
        "\n",
        "    stratum_results = [{'stratum': key,\n",
        "                        main_var_name: np.mean(differences),\n",
        "                        'median': np.median(differences),\n",
        "                        'iqr':  iqr(differences),\n",
        "                        **dict(zip(['ci_lower', 'ci_upper'], IIDBootstrap(np.array(differences)).conf_int(np.mean, 1000, method='percentile'))),\n",
        "                        'total_sample_size': len(data_slices[high_cred_key][key]) + len(data_slices[low_cred_key][key])}\n",
        "                       for key, differences in stratum_boot_differences.items()]\n",
        "\n",
        "    stratum_results_df = pd.DataFrame(stratum_results)\n",
        "\n",
        "    return results_df, stratum_results_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_cwVL1kLvUmD"
      },
      "outputs": [],
      "source": [
        "### Baseline Results - Absolute Values\n",
        "covid_results, covid_results_stratum = stratified_bootstrapping(annotated_data, 'covid_high_cred', 'covid_low_cred', n_iterations=1000,main_var_name='mean_difference', seed=18)\n",
        "climate_results, climate_results_stratum = stratified_bootstrapping(annotated_data, 'climate_high_cred', 'climate_low_cred', n_iterations=1000, main_var_name='mean_difference', seed=18)\n",
        "\n",
        "## Baseline Results - Percentage Values\n",
        "covid_results_percentage, covid_results_stratum_percentage = stratified_bootstrapping(annotated_data, 'covid_high_cred', 'covid_low_cred', n_iterations=1000, main_var_name='mean_difference_perc',seed=18,percentage=True)\n",
        "climate_results_percentage, climate_results_stratum_percentage = stratified_bootstrapping(annotated_data, 'climate_high_cred', 'climate_low_cred', n_iterations=1000, seed=18,main_var_name='mean_difference_perc', percentage=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVaa_L0D-5Vf"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "results_dict = {\n",
        "    'covid_results': covid_results,\n",
        "    'covid_results_stratum': covid_results_stratum,\n",
        "    'climate_results': climate_results,\n",
        "    'climate_results_stratum': climate_results_stratum,\n",
        "    'covid_results_percentage': covid_results_percentage,\n",
        "    'covid_results_stratum_percentage': covid_results_stratum_percentage,\n",
        "    'climate_results_percentage': climate_results_percentage,\n",
        "    'climate_results_stratum_percentage': climate_results_stratum_percentage\n",
        "}\n",
        "\n",
        "with open('file_path/file.pkl', 'wb') as file:\n",
        "    pickle.dump(results_dict, file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFw0T5svkCti"
      },
      "source": [
        "### 4.4 Estimate Impact of Additional Stratification Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq3_loYukPEf"
      },
      "source": [
        "#### 4.4.1 Define Function to Compute the Effect of Additional Stratification Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "a9PM0J151xFe"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def estimate_stratification_effect(stratum_results, stratum_variable_results, stratum_variable, percentage=True):\n",
        "    def split_stratum(df, n, col_name='stratum'):\n",
        "        return [df[col_name].apply(lambda x: x[i] if i < len(x) else np.nan) for i in range(n)]\n",
        "\n",
        "    # Process stratum_results\n",
        "    stratum_results_tmp = stratum_results.copy()\n",
        "    stratum_results_tmp['engagement_stratum'], stratum_results_tmp['followers_stratum'] = split_stratum(stratum_results_tmp, 2)\n",
        "    baseline_avg_diff = stratum_results_tmp.drop(['stratum', 'total_sample_size'], axis=1).groupby(['engagement_stratum', 'followers_stratum']).agg({'mean_difference_perc' if percentage else 'mean_difference': np.mean}).reset_index().rename(columns={'mean_difference_perc' if percentage else 'mean_difference':'baseline_mean_difference'})\n",
        "\n",
        "    # Process stratum_variable_results\n",
        "    stratum_variable_results_tmp = stratum_variable_results.copy()\n",
        "    stratum_variable_results_tmp['engagement_stratum'], stratum_variable_results_tmp['followers_stratum'], stratum_variable_results_tmp[stratum_variable+'_stratum'] = split_stratum(stratum_variable_results_tmp, 3)\n",
        "    avg_diff = stratum_variable_results_tmp.drop(['stratum', 'total_sample_size'], axis=1).groupby(['engagement_stratum', 'followers_stratum', stratum_variable + '_stratum']).agg({'mean_difference_perc' if percentage else 'mean_difference': np.mean}).reset_index()\n",
        "\n",
        "    # Merge and calculate raw difference\n",
        "    result = pd.merge(avg_diff, baseline_avg_diff, on=['engagement_stratum', 'followers_stratum'])\n",
        "    result['impact_of_' + stratum_variable] = result['mean_difference_perc' if percentage else 'mean_difference'] - result['baseline_mean_difference']\n",
        "\n",
        "    # Average raw difference by stratum_variable\n",
        "    mean_raw_difference = result.groupby(stratum_variable+'_stratum')['impact_of_'+stratum_variable].mean().reset_index()\n",
        "\n",
        "    return mean_raw_difference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYpGZC1pqHFa"
      },
      "source": [
        "#### 4.3.2 Additional Stratifications - Toxicity\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "eitE_C7Q3hcV"
      },
      "outputs": [],
      "source": [
        "## Toxicity Results - Absolute Values\n",
        "covid_results_toxicity, covid_results_toxicity_stratum = stratified_bootstrapping(annotated_data, 'covid_high_cred', 'covid_low_cred', n_iterations=1000,main_var_name='mean_difference', seed=18,additional_var=\"toxicity_cluster\")\n",
        "climate_results_toxicity, climate_results_toxicity_stratum = stratified_bootstrapping(annotated_data, 'climate_high_cred', 'climate_low_cred', n_iterations=1000, main_var_name='mean_difference', seed=18,additional_var=\"toxicity_cluster\")\n",
        "\n",
        "## Toxicity Results - Percentage Values\n",
        "covid_results_toxicity_percentage, covid_results_toxicity_stratum_percentage = stratified_bootstrapping(annotated_data, 'covid_high_cred', 'covid_low_cred', n_iterations=1000, main_var_name='mean_difference_perc',seed=18,percentage=True,additional_var=\"toxicity_cluster\")\n",
        "climate_results_toxicity_percentage, climate_results_toxicity_stratum_percentage = stratified_bootstrapping(annotated_data, 'climate_high_cred', 'climate_low_cred', n_iterations=1000, seed=18,main_var_name='mean_difference_perc', percentage=True,additional_var=\"toxicity_cluster\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ip7hN2EVjpQ_"
      },
      "outputs": [],
      "source": [
        "# Compute Effect of Toxicity Stratification\n",
        "covid_effects_tox = estimate_stratification_effect(covid_results_stratum_percentage, covid_results_toxicity_stratum_percentage, \"toxicity\")\n",
        "climate_effects_tox = estimate_stratification_effect(climate_results_stratum_percentage, climate_results_toxicity_stratum_percentage, \"toxicity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RMd6E2qIAlB"
      },
      "source": [
        "#### 4.3.2 Additional Stratifications - Political Bias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "felGBFz03fsr"
      },
      "outputs": [],
      "source": [
        "# Here, we need to clean up the political_bias data, as far-right is not present in high_cred data (as there are no high-credibility far-right domains),\n",
        "# so we need to merge far-right and right, and far-left and left, Also, no_bias sources are ~0.2% in low-credibility data, so we\n",
        "# remove them\n",
        "\n",
        "annotated_data_reduced = {}\n",
        "\n",
        "# Iterate through each key in the original annotated_data dictionary\n",
        "for key in annotated_data:\n",
        "    # Make a copy of the current DataFrame\n",
        "    df = annotated_data[key].copy()\n",
        "\n",
        "    # Group 'right' and 'far-right' together, 'left' and 'far-left' together\n",
        "    df['political_bias'] = df['political_bias'].replace(['right', 'far-right'], 'right_group')\n",
        "    df['political_bias'] = df['political_bias'].replace(['left', 'far-left'], 'left_group')\n",
        "\n",
        "    # Remove rows where political_bias is 'unknown'\n",
        "    df = df[df['political_bias'] != 'unknown']\n",
        "    df = df[df['political_bias'] != 'no_bias']\n",
        "\n",
        "    # Add the processed DataFrame to the new dictionary\n",
        "    annotated_data_reduced[key] = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "accnEhV1eoB0"
      },
      "outputs": [],
      "source": [
        "## Political Bias Results - Absolute Values\n",
        "covid_results_bias, covid_results_bias_stratum= stratified_bootstrapping(annotated_data_reduced, 'covid_high_cred', 'covid_low_cred', n_iterations=1000,main_var_name='mean_difference', seed=18,additional_var=\"political_bias\")\n",
        "climate_results_bias, climate_results_bias_stratum = stratified_bootstrapping(annotated_data_reduced, 'climate_high_cred', 'climate_low_cred', n_iterations=1000, main_var_name='mean_difference', seed=18,additional_var=\"political_bias\")\n",
        "\n",
        "## Political Bias - Percentage Values\n",
        "covid_results_bias_percentage, covid_results_bias_stratum_percentage = stratified_bootstrapping(annotated_data_reduced, 'covid_high_cred', 'covid_low_cred', n_iterations=1000, main_var_name='mean_difference_perc',seed=18,percentage=True,additional_var=\"political_bias\")\n",
        "climate_results_bias_percentage, climate_results_bias_stratum_percentage = stratified_bootstrapping(annotated_data_reduced, 'climate_high_cred', 'climate_low_cred', n_iterations=1000, seed=18,main_var_name='mean_difference_perc', percentage=True,additional_var=\"political_bias\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXg5Xoenkmtj"
      },
      "outputs": [],
      "source": [
        "# Compute Effect of Bias Stratification\n",
        "covid_effects_bias = estimate_stratification_effect(covid_results_stratum_percentage, covid_results_bias_stratum_percentage,'political_bias')\n",
        "climate_effects_bias = estimate_stratification_effect(climate_results_stratum_percentage, climate_results_bias_stratum_percentage,'political_bias')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvgMNSPqIDE8"
      },
      "source": [
        "#### 4.3.3 Additional Stratifications - Verified Status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jRHuhajjw2p"
      },
      "outputs": [],
      "source": [
        "## Verified Results - Absolute Values\n",
        "covid_results_verified, covid_results_verified_stratum= stratified_bootstrapping(annotated_data, 'covid_high_cred', 'covid_low_cred', n_iterations=1000,main_var_name='mean_difference', seed=18,additional_var=\"verified\")\n",
        "climate_results_verified, climate_results_verified_stratum = stratified_bootstrapping(annotated_data, 'climate_high_cred', 'climate_low_cred', n_iterations=1000, main_var_name='mean_difference', seed=18,additional_var=\"verified\")\n",
        "\n",
        "## Verified Results - Percentage Values\n",
        "covid_results_verified_percentage, covid_results_verified_stratum_percentage = stratified_bootstrapping(annotated_data, 'covid_high_cred', 'covid_low_cred', n_iterations=1000, main_var_name='mean_difference_perc',seed=18,percentage=True,additional_var=\"verified\")\n",
        "climate_results_verified_percentage, climate_results_verified_stratum_percentage = stratified_bootstrapping(annotated_data, 'climate_high_cred', 'climate_low_cred', n_iterations=1000, seed=18,main_var_name='mean_difference_perc', percentage=True,additional_var=\"verified\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqxbKEPw_qze"
      },
      "outputs": [],
      "source": [
        "# Compute Effect of Bias Stratification\n",
        "covid_effects_verified = estimate_stratification_effect(covid_results_stratum_percentage, covid_results_verified_stratum_percentage,'verified',percentage=True)\n",
        "climate_effects_verified = estimate_stratification_effect(climate_results_stratum_percentage, climate_results_verified_stratum_percentage,'verified',percentage=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JedRQ0yxGD6Z"
      },
      "source": [
        "#### 4.3.4 Export Stratifications Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gi3UwOx8DTGE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "def prepare_stratification_data(prefix):\n",
        "    df1 = globals()[f'{prefix}_effects_bias'].rename(columns={\"political_bias_stratum\": \"stratum\", \"impact_of_political_bias\": \"impact\"})\n",
        "    df1['stratum_type'] = 'political_bias'\n",
        "\n",
        "    df2 = globals()[f'{prefix}_effects_verified'].rename(columns={\"verified_stratum\": \"stratum\", \"impact_of_verified\": \"impact\"})\n",
        "    df2['stratum_type'] = 'verified'\n",
        "\n",
        "    df3 = globals()[f'{prefix}_effects_tox'].rename(columns={\"toxicity_stratum\": \"stratum\", \"impact_of_toxicity\": \"impact\"})\n",
        "    df3['stratum_type'] = 'toxicity'\n",
        "\n",
        "    df = pd.concat([df1, df2, df3])\n",
        "    df['stratum'] = df['stratum'].astype(str)  # ensure stratum is string type\n",
        "\n",
        "    # Modify labels\n",
        "    label_mapping = {\n",
        "        \"True\": \"verified_true\",\n",
        "        \"False\": \"verified_false\",\n",
        "        \"right_group\": \"political_bias_right\",\n",
        "        \"left_group\": \"political_bias_left\",\n",
        "        \"0\": \"toxicity_low\",\n",
        "        \"1\": \"toxicity_mid\",\n",
        "        \"2\": \"toxicity_high\"\n",
        "    }\n",
        "\n",
        "    df['stratum'] = df['stratum'].replace(label_mapping)\n",
        "\n",
        "    return df\n",
        "\n",
        "covid_stratifications = prepare_stratification_data('covid')\n",
        "climate_stratifications = prepare_stratification_data('climate')\n",
        "\n",
        "results_dict = {\n",
        "    'covid_stratifications': covid_stratifications,\n",
        "    'climate_stratifications': climate_stratifications\n",
        "}\n",
        "\n",
        "\n",
        "with open('file_path/file.pkl', 'wb') as file:\n",
        "    pickle.dump(results_dict, file)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "cIQv9MXpYIgB",
        "e2Jftp5c-wrg",
        "Q_SjjDwD-365",
        "kw1O7Xt1-7tm",
        "hccLEXFX-WlA",
        "DV0VNq7Dav6z",
        "dla82RAofhuI",
        "bDw5XslsgKBh",
        "ePR-vlOUTiE5",
        "ZBrKkPP51gMh"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
